name: divbase
networks:
  # Simulate S3 living outside remainder of services
  internal:
    driver: bridge
  external-s3:
    driver: bridge
  # External network for monitoring services defined in monitoring_compose.yaml
  divbase-observability:
    name: divbase-observability
    driver: bridge
services:
  rabbitmq:
    image: "rabbitmq:4.2.1-management-alpine"
    healthcheck:
      test: ["CMD", "rabbitmqctl", "status"]
      interval: 5s
      timeout: 5s
      retries: 5
    ports:
      - 5672:5672
      - 15692:15692 # Prometheus metrics endpoint
    environment:
      - RABBITMQ_DEFAULT_USER=celery_user
      - RABBITMQ_DEFAULT_PASS=badpassword
      - RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=-rabbitmq_prometheus true
      - RABBITMQ_PROMETHEUS_RETURN_PER_OBJECT_METRICS=true # enable per-queue metrics
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq #to avoid anonymous volumes being created
    networks: &internal-network
      - internal 
      - divbase-observability
  worker-quick:
    hostname: worker-quick
    build: &worker-build
      context: .. 
      dockerfile: docker/worker.dockerfile
    healthcheck: &worker-healthcheck
      test: ["CMD", "celery", "inspect", "ping"]
      interval: 10s
      timeout: 10s
      retries: 3
      start_period: 10s
    depends_on: &worker-api-depends
      rabbitmq:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio-setup:
        condition: service_completed_successfully
      db-migrator:
        condition: service_completed_successfully
    environment: &worker-env
      - CELERY_BROKER_URL=pyamqp://celery_user:badpassword@rabbitmq:5672//
      - CELERY_RESULT_BACKEND=db+postgresql://divbase_user:badpassword@postgres:5432/divbase_db
      - SYNC_DATABASE_URL=postgresql+psycopg2://divbase_user:badpassword@postgres:5432/divbase_db
      - S3_ENDPOINT_URL=http://host.docker.internal:9000
      - S3_PRESIGNING_URL=http://localhost:9000
      - S3_SERVICE_ACCOUNT_ACCESS_KEY=s3-service-account
      - S3_SERVICE_ACCOUNT_SECRET_KEY=badpassword
    develop: &watch-celery-src
      watch:
        - action: sync+restart
          path: ../packages/divbase-api/src/divbase_api/
          target: /usr/local/lib/python3.12/site-packages/divbase_api/
        - action: sync+restart
          path: ../packages/divbase-lib/src/divbase_lib/
          target: /usr/local/lib/python3.12/site-packages/divbase_lib/
        - action: rebuild
          path: ../uv.lock
    command: ["-Q", "quick,celery", "--hostname=worker-quick@%h", "--concurrency=4", "-B"] #-Q "celery" is the default queue, so let the quick worker also handle it to avoid issues with tasks not being picked up
    networks: *internal-network
    ports:
      - "8001:8001" # for prometheus metrics scraping; need to have unique port per worker to build compose stack, but can then scrape based on the service name and port 8001
  worker-long:
    hostname: worker-long
    build: *worker-build
    healthcheck: *worker-healthcheck
    depends_on: *worker-api-depends
    environment: *worker-env
    develop: *watch-celery-src
    command: ["-Q", "long", "--hostname=worker-long@%h", "--concurrency=4"]
    networks: *internal-network
    ports:
      - "8002:8001" # for prometheus metrics scraping; need to have unique port per worker to build compose stack, but can then scrape based on the service name and port 8001
  minio:
    image: bitnamilegacy/minio:2025.7.23-debian-12-r5
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: badpassword
    command: server --console-address ":9001" /data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 3s
      retries: 5
    user: "0:0" #for local dev only! To avoid volume permission issues with Docker on macOS
    volumes:
      - minio-data:/data
      - minio-bitnami-data:/bitnami/minio/data #to avoid anonymous volumes being created
      - minio-certs:/certs #to avoid anonymous volumes being created
    networks: &s3-network
      - external-s3
  minio-setup: # Create the service account 
    image: bitnamilegacy/minio-client:2025.7.21-debian-12-r3
    depends_on:
      minio:
        condition: service_healthy
    restart: on-failure
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=badpassword
      - SERVICE_ACCOUNT_ACCESS_KEY=s3-service-account
      - SERVICE_ACCOUNT_SECRET_KEY=badpassword
    volumes:
      - ./s3_service_account_policy.json:/tmp/minio-policy.json:ro
    networks: *s3-network
    entrypoint: >
      /bin/sh -c "
      mc alias set minio http://minio:9000 minioadmin badpassword;
      mc admin policy create minio divbase-policy /tmp/minio-policy.json;
      mc admin user add minio s3-service-account badpassword;
      mc admin policy attach minio divbase-policy --user s3-service-account;
      "
  db-migrator: # init container that runs any pending alembic migrations on startup
    build:
      context: ..
      dockerfile: docker/fastapi.dockerfile
    environment:
      - SYNC_DATABASE_URL=postgresql+psycopg2://divbase_user:badpassword@postgres:5432/divbase_db
    command: ["alembic", "upgrade", "head"]
    depends_on: 
      postgres:
        condition: service_healthy
    networks: *internal-network
    volumes: # read only volumes to get any new migration files (otherwise have to rebuild image to get new migration scripts)
      - ../packages/divbase-api/src/divbase_api/migrations:/app/migrations:ro
      - ../packages/divbase-api/src/divbase_api/alembic.ini:/app/alembic.ini:ro
  fastapi:
    build:
      context: ..
      dockerfile: docker/fastapi.dockerfile
    ports:
      - 8000:8000
    command: ["fastapi", "dev", "--host", "0.0.0.0", "/usr/local/lib/python3.12/site-packages/divbase_api/divbase_api.py"]
    develop: # rebuild only on changes to deps. 
      watch:
        - action: sync+restart
          path: ../packages/divbase-api/src/divbase_api/
          target: /usr/local/lib/python3.12/site-packages/divbase_api/
        - action: sync+restart
          path: ../packages/divbase-lib/src/divbase_lib/
          target: /usr/local/lib/python3.12/site-packages/divbase_lib/
        - action: rebuild
          path: ../uv.lock
    volumes:
      - ../packages/divbase-api/src/divbase_api/migrations:/app/migrations # this is to get the migration scripts which are generated inside the container 
      - ../packages/divbase-api/src/divbase_api/alembic.ini:/app/alembic.ini
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 5s
      timeout: 5s
      retries: 5
    environment:
    - DIVBASE_ENV=local_dev
    - FRONTEND_BASE_URL=http://localhost:8000
    - MKDOCS_SITE_URL=http://localhost:8008/divbase 
    - CELERY_BROKER_URL=pyamqp://celery_user:badpassword@rabbitmq:5672//
    - CELERY_RESULT_BACKEND=db+postgresql://divbase_user:badpassword@postgres:5432/divbase_db
    - ASYNC_DATABASE_URL=postgresql+asyncpg://divbase_user:badpassword@postgres:5432/divbase_db 
    - SYNC_DATABASE_URL=postgresql+psycopg2://divbase_user:badpassword@postgres:5432/divbase_db # Needed for lifespan event to check migrations are at HEAD.
    - JWT_SECRET_KEY=a-string-secret-at-least-256-bits-long
    - FIRST_ADMIN_EMAIL=admin@divbase.com
    - FIRST_ADMIN_PASSWORD=badpassword
    - S3_ENDPOINT_URL=http://host.docker.internal:9000
    - S3_PRESIGNING_URL=http://localhost:9000
    - S3_SERVICE_ACCOUNT_ACCESS_KEY=s3-service-account
    - S3_SERVICE_ACCOUNT_SECRET_KEY=badpassword
    depends_on: *worker-api-depends
    networks: *internal-network
  postgres:
    image: postgres:17
    ports:
      - "5432:5432"
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U divbase_user -d divbase_db"]
      interval: 10s
      retries: 5
      start_period: 30s
      timeout: 10s
    volumes:
      - postgres-data:/var/lib/postgresql/data/pgdata
    environment:
      - PGDATA=/var/lib/postgresql/data/pgdata
      - POSTGRES_USER=divbase_user
      - POSTGRES_DB=divbase_db
      - POSTGRES_PASSWORD=badpassword
    networks: *internal-network
  mailpit:
    image: axllent/mailpit:v1.27
    ports:
      - "1025:1025"  # SMTP port
      - "8025:8025"  # Web UI 
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8025/"]
      interval: 10s
      timeout: 5s
      retries: 3
    volumes:
      - mailpit-data:/data
    networks: *internal-network
volumes:
  rabbitmq-data:
  minio-data:
  minio-bitnami-data:
  minio-certs:
  postgres-data:
  mailpit-data: